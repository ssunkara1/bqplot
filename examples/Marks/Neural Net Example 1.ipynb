{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bqplot import *\n",
    "from bqplot.marks import Graph\n",
    "from ipywidgets import IntSlider, Dropdown, RadioButtons, HBox, VBox, Button\n",
    "from bqplot import pyplot as plt\n",
    "from bqplot import OrdinalScale\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "try:\n",
    "    _ = seed\n",
    "except NameError:\n",
    "    seed = None\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "data_root = 'E:/Code/'\n",
    "data_df_total = pd.read_csv(data_root + 'Data/Credit Data/cs-training.csv', \n",
    "                            index_col=0)\n",
    "result_column = 'SeriousDlqin2yrs'\n",
    "\n",
    "train_idx, test_idx = train_test_split(data_df_total.index.values, test_size=0.3,\n",
    "                                       stratify=data_df_total[result_column]\n",
    "                                       )\n",
    "strat_cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_data = data_df_total.loc[train_idx]\n",
    "test_data = data_df_total.loc[test_idx]\n",
    "\n",
    "try:\n",
    "    _ = reset_seed\n",
    "except NameError:\n",
    "    reset_seed = False\n",
    "\n",
    "if reset_seed:\n",
    "    np.random.seed(None)   \n",
    "\n",
    "#%%\n",
    "impute_income = False\n",
    "stack_models = False\n",
    "fill_smart = False\n",
    "fit_expanded = False\n",
    "group_models = False\n",
    "rescale_models = False\n",
    "plot_figure = True\n",
    "\n",
    "# %%\n",
    "overdue_cols = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate']\n",
    "rev_lines_col = 'RevolvingUtilizationOfUnsecuredLines'\n",
    "\n",
    "def clean_data_for_prediction(data_frame):\n",
    "    # remove all nan monthly incomes\n",
    "    # remove NumberOfDaysLate >= 90.\n",
    "    # remove Revolving Utilization of Credit Lines >= 4.\n",
    "    # remove obscene values of DebtRatio.\n",
    "    # income greater than 1.\n",
    "    reduced_df = data_frame.copy()\n",
    "    reduced_df = reduced_df[~reduced_df['MonthlyIncome'].isnull()]\n",
    "    reduced_df = reduced_df[reduced_df['MonthlyIncome'] > 100.]\n",
    "    \n",
    "    for c in overdue_cols:\n",
    "        reduced_df = reduced_df[reduced_df[c] <= 90.]\n",
    "    \n",
    "    reduced_df = reduced_df[reduced_df[rev_lines_col] <= 4.]\n",
    "    return reduced_df\n",
    "\n",
    "def train_income_model(data_frame):\n",
    "    col_train_data = clean_data_for_prediction(data_frame)\n",
    "    col_train_X, col_train_y = col_train_data.drop(['MonthlyIncome', 'DebtRatio', result_column], axis=1), col_train_data['MonthlyIncome']\n",
    "    col_train_X = col_train_X.fillna(data_median)\n",
    "    \n",
    "    col_model = GradientBoostingRegressor(n_estimators=300, max_depth=7, max_features=4, \n",
    "                                          learning_rate=0.1,\n",
    "                                          min_weight_fraction_leaf=0.0001)\n",
    "    col_model.fit(col_train_X, col_train_y)\n",
    "    return col_model\n",
    "\n",
    "#%%:\n",
    "def clean_train_data(train_df):\n",
    "    train_df = train_df.copy()\n",
    "    \n",
    "    data_median = train_df.median()\n",
    "    nan_income = train_df.index[np.logical_or(train_df['MonthlyIncome'].isnull(), train_df['MonthlyIncome'] < 100.)]\n",
    "\n",
    "    if impute_income:\n",
    "        test_income_data = train_df.loc[nan_income].drop(['MonthlyIncome', 'DebtRatio', result_column], axis=1)\n",
    "        test_income_data = test_income_data.fillna(data_median)\n",
    "        income_fill_values = income_model.predict(test_income_data)\n",
    "    \n",
    "        train_df.loc[nan_income, 'MonthlyIncome'] = income_fill_values\n",
    "        train_df.loc[nan_income, 'DebtRatio'] = train_df.loc[nan_income, 'DebtRatio'] / income_fill_values       \n",
    "    else:\n",
    "        train_df.loc[nan_income, 'MonthlyIncome'] = data_median['MonthlyIncome']\n",
    "        train_df.loc[nan_income, 'DebtRatio'] = train_df.loc[nan_income, 'DebtRatio'] / data_median['MonthlyIncome']\n",
    "\n",
    "    fill_values = {}\n",
    "    fill_values['MonthlyIncome'] = data_median['MonthlyIncome']\n",
    "    \n",
    "    for col in overdue_cols:\n",
    "        num_overdue_df = train_df.loc[train_df[col] >= 90]\n",
    "        if fill_smart:\n",
    "            over_due_fill_values = train_df.loc[~train_df.index.isin(num_overdue_df.index)].groupby(result_column).mean()\n",
    "            fill_values[col] = over_due_fill_values[col].mean()\n",
    "            train_df.loc[num_overdue_df.index, col] = train_df.loc[num_overdue_df.index, result_column].map(lambda x: over_due_fill_values.loc[x, col])\n",
    "        else:                \n",
    "            fill_values[col] = train_df[col].median()\n",
    "            train_df.loc[num_overdue_df.index, col] = train_df[col].median()\n",
    " \n",
    "    ## filling the value for revolving unsecured lines.\n",
    "    rev_filtered_df = train_df[train_df[rev_lines_col] >= 4.0]\n",
    "    if fill_smart:\n",
    "        rev_fill_values = train_df.loc[~train_df.index.isin(rev_filtered_df.index)].groupby(result_column).median()\n",
    "        fill_values[rev_lines_col] = rev_fill_values[rev_lines_col].mean()\n",
    "        train_df.loc[rev_filtered_df.index, rev_lines_col] = train_df.loc[rev_filtered_df.index, result_column].map(lambda x: rev_fill_values.loc[x, rev_lines_col])\n",
    "    else:    \n",
    "        train_df.loc[rev_filtered_df.index, rev_lines_col] = train_df[rev_lines_col].median()\n",
    "        fill_values[rev_lines_col] = train_df[rev_lines_col].median()\n",
    "    return train_df, fill_values \n",
    "\n",
    "\n",
    "def clean_test_data(test_df, fill_values, fill_values_other):\n",
    "    test_df = test_df.copy()\n",
    "    nan_income_idxs = test_df.index[np.logical_or(test_df['MonthlyIncome'].isnull(), test_df['MonthlyIncome'] < 100.)]\n",
    "    \n",
    "    if impute_income:\n",
    "        test_income_data = test_df.loc[nan_income_idxs].drop(['MonthlyIncome', 'DebtRatio', result_column], axis=1)\n",
    "        test_income_data = test_income_data.fillna(fill_values_other)\n",
    "        income_fill_values = income_model.predict(test_income_data)   \n",
    "        test_df.loc[nan_income_idxs, 'MonthlyIncome'] = income_fill_values\n",
    "        test_df.loc[nan_income_idxs, 'DebtRatio'] = test_df.loc[nan_income_idxs, 'DebtRatio'] / income_fill_values               \n",
    "    else:\n",
    "        test_df.loc[nan_income_idxs, 'MonthlyIncome'] = fill_values['MonthlyIncome']\n",
    "        test_df.loc[nan_income_idxs, 'DebtRatio'] = test_df.loc[nan_income_idxs, 'DebtRatio'] / fill_values['MonthlyIncome']\n",
    "\n",
    "    for c in overdue_cols:\n",
    "        fill_idxs = test_df.index[test_df[c] >= 90]\n",
    "        test_df.loc[fill_idxs, c] = fill_values[c]\n",
    "\n",
    "    fill_rev_idxs = test_df.index[test_df[rev_lines_col] >= 4.0]\n",
    "    test_df.loc[fill_rev_idxs, rev_lines_col] = fill_values[rev_lines_col]\n",
    "\n",
    "    test_df = test_df.fillna(fill_values_other)\n",
    "    return test_df\n",
    "\n",
    "def add_features(data_frame):\n",
    "    return_dataframe = data_frame.copy()\n",
    "    return_dataframe[rev_lines_col+'ind'] = return_dataframe[rev_lines_col] == 0.\n",
    "    return_dataframe['overdue_ind'] = (return_dataframe[overdue_cols].sum(axis=1) == 0)\n",
    "    return return_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140fd27f834740f6a01887ce13a0b4d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20s - loss: 0.5919 - acc: 0.7982\n",
      "Epoch 2/15\n",
      "11s - loss: 0.4643 - acc: 0.9229\n",
      "Epoch 3/15\n",
      "10s - loss: 0.3644 - acc: 0.9314\n",
      "Epoch 4/15\n",
      "5s - loss: 0.2935 - acc: 0.9320\n",
      "Epoch 5/15\n",
      "5s - loss: 0.2506 - acc: 0.9327\n",
      "Epoch 6/15\n",
      "5s - loss: 0.2278 - acc: 0.9330\n",
      "Epoch 7/15\n",
      "5s - loss: 0.2156 - acc: 0.9335\n",
      "Epoch 8/15\n",
      "5s - loss: 0.2087 - acc: 0.9334\n",
      "Epoch 9/15\n",
      "5s - loss: 0.2044 - acc: 0.9340\n",
      "Epoch 10/15\n",
      "5s - loss: 0.2016 - acc: 0.9342\n",
      "Epoch 11/15\n",
      "5s - loss: 0.1996 - acc: 0.9346\n",
      "Epoch 12/15\n",
      "5s - loss: 0.1979 - acc: 0.9347\n",
      "Epoch 13/15\n",
      "5s - loss: 0.1961 - acc: 0.9351\n",
      "Epoch 14/15\n",
      "5s - loss: 0.1954 - acc: 0.9350\n",
      "Epoch 15/15\n",
      "5s - loss: 0.1940 - acc: 0.9348\n",
      "       accuracy       auc  precision    recall\n",
      "Train  0.935771  0.843202   0.566764  0.165717\n",
      "Test   0.936867  0.837519   0.600000  0.166556\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data_median = train_data.median()\n",
    "if impute_income:\n",
    "    income_model = train_income_model(train_data)\n",
    "\n",
    "train_data_clean, fill_dict = clean_train_data(train_data)\n",
    "data_median = train_data_clean.median()\n",
    "\n",
    "## fill in the remaining values with the median\n",
    "train_data_clean = train_data_clean.fillna(data_median)\n",
    "train_data_clean = add_features(train_data_clean)\n",
    "\n",
    "test_data_cleaned = clean_test_data(test_data, fill_dict, data_median)\n",
    "test_data_cleaned = add_features(test_data_cleaned)\n",
    "\n",
    "X_train = train_data_clean.drop(result_column, axis=1)\n",
    "y_train = train_data_clean[result_column]\n",
    "\n",
    "X_test = test_data_cleaned.drop(result_column, axis=1)\n",
    "y_test = test_data_cleaned[result_column]\n",
    "\n",
    "#%%\n",
    "num_epochs = 15\n",
    "batch_size = 5000\n",
    "\n",
    "#%%\n",
    "## common utility functions\n",
    "def eval_preds(y_true, y_probs, y_preds):\n",
    "    return {'precision': precision_score(y_true, y_preds),\n",
    "            'accuracy': accuracy_score(y_true, y_preds),\n",
    "            'recall': recall_score(y_true, y_preds),\n",
    "            'auc': roc_auc_score(y_true, y_probs)}\n",
    "\n",
    "def get_model_eval(true_train, train_predictions, true_test=None, test_predictions=None):\n",
    "    train_eval = eval_preds(true_train, *train_predictions)\n",
    "    if true_test is None:\n",
    "        return pd.Series(train_eval)\n",
    "    else:\n",
    "        test_eval = eval_preds(true_test, *test_predictions)\n",
    "        return pd.DataFrame([train_eval, test_eval], index=['Train', 'Test'])\n",
    "    \n",
    "def get_sample_weights(y_train, power=1.0):\n",
    "    y_train = pd.Series(y_train)\n",
    "    return y_train.map(1. - (y_train.value_counts() / len(y_train))) ** power    \n",
    "\n",
    "def probas_to_classes(probas):\n",
    "    return (probas >= 0.5).astype(float)\n",
    "\n",
    "#%%\n",
    "from keras.models import Sequential\n",
    "# from keras.utils.np_utils import probas_to_classes\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)\n",
    "\n",
    "sample_weights = np.ones(X_train.shape[0])\n",
    "class WeightsGradientsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.gradients = []\n",
    "        self.train_auc = []\n",
    "        self.test_auc = []\n",
    "        self.weights = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        input_values = [X_train_norm, sample_weights, y_train.reshape(-1, 1), 0]\n",
    "        gradient_values = compute_gradients(input_values)\n",
    "        self.gradients.append(gradient_values)\n",
    "        \n",
    "        self.train_auc.append(roc_auc_score(y_train.values.flatten(), \n",
    "                                            self.model.predict(X_train_norm)))\n",
    "        self.test_auc.append(roc_auc_score(y_test.values.flatten(), \n",
    "                                           self.model.predict(X_test_norm)))\n",
    "        \n",
    "        auc_line.x = np.arange(0, epoch + 1)\n",
    "        auc_line.y = [self.train_auc, self.test_auc]\n",
    "        \n",
    "        weights = list(range(len(self.model.layers)))\n",
    "        for i, l in enumerate(self.model.layers):\n",
    "            weights[i] = l.get_weights()\n",
    "        self.weights.append(weights)\n",
    "\n",
    "test_call_back = WeightsGradientsCallback()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_norm = scaler.fit_transform(X_train.values)\n",
    "X_test_norm = scaler.transform(X_test.values)\n",
    "dropout_prob = 0.15\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=X_train_norm.shape[1], activation='relu'))\n",
    "model.add(Dropout(dropout_prob))\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "gradients = model.optimizer.get_gradients(model.model.total_loss, \n",
    "                                          model.trainable_weights)\n",
    "input_tensors = [model.model.inputs[0], model.model.sample_weights[0], \n",
    "                 model.model.targets[0], K.learning_phase()]\n",
    "compute_gradients = K.function(inputs=input_tensors, outputs=gradients)   \n",
    "\n",
    "\n",
    "auc_fig = plt.figure(title='Train and Test AUC vs epoch', legend_location='top-left')\n",
    "auc_line = plt.plot([0], [0], marker='circle', marker_size=32, colors=['DeepSkyBlue', 'Red'], \n",
    "                              labels=['Training', 'Test'], display_legend=True)\n",
    "display(auc_fig)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_norm, y_train.values, verbose=2,\n",
    "          epochs=num_epochs, batch_size=batch_size,\n",
    "           callbacks=[test_call_back])\n",
    "\n",
    "train_probs = model.predict(X_train_norm).flatten()\n",
    "train_preds = probas_to_classes(train_probs)\n",
    "\n",
    "test_probs = model.predict(X_test_norm).flatten()\n",
    "test_preds = probas_to_classes(test_probs)\n",
    "\n",
    "\n",
    "model_eval = get_model_eval(y_train, [train_probs, train_preds],\n",
    "                            y_test, [test_probs, test_preds])\n",
    "print(model_eval)\n",
    "    \n",
    "sess.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_cleaned_weights(weights_mat):\n",
    "    weights_ret = []\n",
    "    for w in weights_mat:\n",
    "        if np.shape(w)[0] == 0:\n",
    "            # this is a dropout layer or a reg layer which does no have weights\n",
    "            pass\n",
    "        else:\n",
    "            weights_ret.append(w)\n",
    "    return weights_ret\n",
    "\n",
    "def get_weights_for_node_at_layer(weights, epoch_num, layer_num, node_num):\n",
    "    # max_layers = len(weights)\n",
    "    layer_params = weights[epoch_num][layer_num]\n",
    "    \n",
    "    layer_weights = layer_params[0]\n",
    "    layer_bias = layer_params[1]\n",
    "    \n",
    "    node_weights = layer_weights[:, node_num]\n",
    "    node_bias = layer_bias[node_num]\n",
    "    \n",
    "    return (node_bias, node_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_gradients_for_node_at_layer(gradients, epoch_num, layer_num, node_num):\n",
    "    layer_gradients = gradients[epoch_num][2 * layer_num]\n",
    "    layer_bias_gradients = gradients[epoch_num][2 * layer_num + 1]\n",
    "    \n",
    "    node_gradients = layer_gradients[:, node_num]\n",
    "    node_bias_gradiens = layer_bias_gradients[node_num]\n",
    "    \n",
    "    return(node_bias_gradiens, node_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cleaned_weights = []\n",
    "\n",
    "for w in test_call_back.weights:\n",
    "    cleaned_weights.append(get_cleaned_weights(w))\n",
    "# get_cleaned_weights(test_call_back.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain, product\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.height = kwargs.get('height', 1000)\n",
    "        self.width = kwargs.get('width', 1200)\n",
    "        self.layer_colors = kwargs.get('layer_colors', CATEGORY10)\n",
    "        self.directed_links = kwargs.get('directed_links', False)\n",
    "        \n",
    "        self.nodes_input_layer = kwargs['nx']\n",
    "        self.nodes_hidden_layers = kwargs['hidden_layers']\n",
    "        self.nodes_output_layer = kwargs['ny']\n",
    "        \n",
    "        self.build_net()\n",
    "    \n",
    "    def build_net(self):\n",
    "        # create nodes\n",
    "        self.layer_nodes = []\n",
    "        self.layer_nodes.append(['x' + str(i+1) for i in range(self.nodes_input_layer)])\n",
    "        \n",
    "        for i, h in enumerate(self.nodes_hidden_layers):\n",
    "            self.layer_nodes.append(['h' + str(i+1) + ',' + str(j+1) for j in range(h)])\n",
    "        self.layer_nodes.append(['y' + str(i+1) for i in range(self.nodes_output_layer)])\n",
    "        \n",
    "        self.flattened_layer_nodes = list(chain(*self.layer_nodes))\n",
    "        \n",
    "        # build link matrix\n",
    "        i = 0\n",
    "        node_indices = {}\n",
    "        for layer in self.layer_nodes:\n",
    "            for node in layer:\n",
    "                node_indices[node] = i\n",
    "                i += 1\n",
    "\n",
    "        n = len(self.flattened_layer_nodes)\n",
    "        self.link_matrix = np.empty((n,n))\n",
    "        self.link_matrix[:] = np.nan\n",
    "\n",
    "        for i in range(len(self.layer_nodes) - 1):\n",
    "            curr_layer_nodes_indices = [node_indices[d] for d in self.layer_nodes[i]]\n",
    "            next_layer_nodes = [node_indices[d] for d in self.layer_nodes[i+1]]\n",
    "            for s, t in product(curr_layer_nodes_indices, next_layer_nodes):\n",
    "                self.link_matrix[s, t] = 1\n",
    "        \n",
    "        # set node x locations\n",
    "        self.nodes_x = np.repeat(np.linspace(0, 100, \n",
    "                                             len(self.layer_nodes) + 1, \n",
    "                                             endpoint=False)[1:], \n",
    "                                 [len(n) for n in self.layer_nodes])\n",
    "\n",
    "        # set node y locations\n",
    "        self.nodes_y = np.array([])\n",
    "        for layer in self.layer_nodes:\n",
    "            n = len(layer)\n",
    "            ys = np.linspace(0, 100, n+1, endpoint=False)[1:]\n",
    "            self.nodes_y = np.append(self.nodes_y, ys[::-1])\n",
    "        \n",
    "        # set node colors\n",
    "        n_layers = len(self.layer_nodes)\n",
    "        self.node_colors = np.repeat(np.array(self.layer_colors[:n_layers]), \n",
    "                                     [len(layer) for layer in self.layer_nodes]).tolist()\n",
    "        \n",
    "        xs = LinearScale(min=0, max=100)\n",
    "        ys = LinearScale(min=0, max=100)\n",
    "        \n",
    "        self.graph = Graph(node_labels=self.flattened_layer_nodes, \n",
    "                           link_matrix=self.link_matrix, \n",
    "                           link_type='line',\n",
    "                           colors=self.node_colors,\n",
    "                           directed=self.directed_links,\n",
    "                           scales={'x': xs, 'y': ys}, \n",
    "                           x=self.nodes_x, \n",
    "                           y=self.nodes_y)\n",
    "        self.graph.hovered_style = {'opacity': '1', \n",
    "                                    'stroke': 'white'}\n",
    "        \n",
    "        self.graph.selected_style = {'opacity': '1',\n",
    "                                     'stroke': 'white',\n",
    "                                     'stroke-width': '2.5'}\n",
    "        \n",
    "        self.fig = Figure(marks=[self.graph])\n",
    "        self.fig.layout.width = str(self.width) + 'px'\n",
    "        self.fig.layout.height = str(self.height) + 'px'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNet(nx=12, hidden_layers=[20, 10], ny=1)\n",
    "\n",
    "epoch_slider = IntSlider(description='Epoch:', min=1, max=num_epochs, value=1)\n",
    "mode_dd = Dropdown(description='View', options=['Weights', 'Gradients'], value='Weights')\n",
    "agg_radio = RadioButtons(description='Aggregation', options=['Nodes', 'Layers'], value='Nodes')\n",
    "update_btn = Button(description='Update')\n",
    "\n",
    "bar_figure = plt.figure()\n",
    "bar_plot = plt.bar([], [], scales={'x': OrdinalScale()})\n",
    "\n",
    "controls = HBox([epoch_slider, mode_dd, agg_radio, update_btn])\n",
    "\n",
    "nn.graph.tooltip = bar_figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b5e132a834ef498af0c957674bacc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_bar_chart(layer, node):\n",
    "    epoch = epoch_slider.value\n",
    "    \n",
    "    if mode_dd.value == 'Weights':\n",
    "        display_vals = get_weights_for_node_at_layer(cleaned_weights, epoch, layer-1, node)\n",
    "    else:\n",
    "        display_vals = get_gradients_for_node_at_layer(test_call_back.gradients, epoch, layer-1, node)\n",
    "    return_vals = np.append([display_vals[0]], display_vals[1])\n",
    "    \n",
    "    bar_figure.title = mode_dd.value + ' for layer:' + str(layer) + ' node: ' + str(node) + ' at epoch: ' + str(epoch)\n",
    "    bar_plot.x = np.arange(len(return_vals))\n",
    "    bar_plot.y = return_vals\n",
    "    \n",
    "node_counts = [nn.nodes_input_layer] + nn.nodes_hidden_layers + [nn.nodes_output_layer]\n",
    "\n",
    "def hovered_change(change):\n",
    "    point_index = change['new']\n",
    "    if point_index is None:\n",
    "        return\n",
    "    else:\n",
    "        for i, n in enumerate(node_counts):\n",
    "            if point_index < n:\n",
    "                break\n",
    "            else:\n",
    "                point_index = point_index - n\n",
    "        if i > 0:\n",
    "            update_bar_chart(i, point_index)\n",
    "    \n",
    "nn.graph.observe(hovered_change, 'hovered_point')\n",
    "\n",
    "VBox([controls, nn.fig])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "input_collapsed": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
