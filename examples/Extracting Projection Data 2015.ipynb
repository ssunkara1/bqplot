{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "position_codes = [1, 2, 3, 4, 7, 8]\n",
    "position_idx = 1\n",
    "csv_file_name = 'qb_proj_2015.csv'\n",
    "row_class_name = 'projected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this has to be programmatically constructed to fetch data for various positions and\n",
    "## for different years etc.\n",
    "url_prefix = 'http://fantasy.nfl.com/research/projections?position='\n",
    "url_suffix = '&statCategory=projectedStats&statSeason=2015&statType=seasonProjectedStats&statWeek=1'\n",
    "\n",
    "url_link = url_prefix + str(position_idx) + url_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## read data from url and extract teh table\n",
    "# response = request.urlopen(url_link)\n",
    "# html = response.read()\n",
    "\n",
    "# soup = BeautifulSoup(html, 'lxml')\n",
    "# table = soup.find('div', {'class': 'tableWrap'})\n",
    "\n",
    "# if table is None:\n",
    "#     print('Table not found. url may be misconstructed.')\n",
    "# else:\n",
    "#     print('Table Found. Good to go ahead.')\n",
    "\n",
    "# table_body = table.find('tbody')\n",
    "# table_rows = table_body.findAll('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_table_rows(prefix, index, suffix):\n",
    "    current_url = prefix + str(index) + suffix\n",
    "    \n",
    "    response = request.urlopen(current_url)\n",
    "    html = response.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    table = soup.find('div', {'class': 'tableWrap'})\n",
    "\n",
    "    if table is None:\n",
    "        # print('Table not found. url may be misconstructed.')\n",
    "        table_rows = []\n",
    "    else:\n",
    "        # print('Table Found. Good to go ahead.')\n",
    "        table_body = table.find('tbody')\n",
    "        table_rows = table_body.findAll('tr')\n",
    "    \n",
    "    return table_rows\n",
    "\n",
    "\n",
    "def get_data_frame_from_rows(rows):\n",
    "    data = {}\n",
    "    for row in rows:\n",
    "        ## get projected score\n",
    "        ## have to change for historic data\n",
    "        score = row.findAll('td', {'class': row_class_name})[0].string.strip()\n",
    "        name_card = row.findAll('td', {'class', 'playerNameAndInfo'})[0].find('div')\n",
    "\n",
    "        player_name = name_card.find('a', {'class': 'playerName'}).string.strip()\n",
    "        pos_team = name_card.find('em').string.strip().split('-')\n",
    "        pos, team = pos_team[0].strip(), '' if len(pos_team) == 1 else pos_team[1].strip()\n",
    "\n",
    "        data[player_name] = {'pos': pos, 'team': team, 'score': score}\n",
    "        \n",
    "    data_frame = pd.DataFrame.from_dict(data, orient='index')\n",
    "    data_frame['score'] = data_frame['score'].astype('float')\n",
    "        \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create urls for different pages.\n",
    "# initially, we are on the first page. Assume the results are 25 entires long.\n",
    "data_frames = []\n",
    "iter = 0\n",
    "iterate = True\n",
    "\n",
    "while(iterate):\n",
    "    offset_suffix = ''\n",
    "    if iter > 0:\n",
    "        offset_suffix = '&offset=' + str((iter*25)+1)\n",
    "    iter_suffix = url_suffix + offset_suffix\n",
    "    table_rows = get_table_rows(url_prefix, position_idx, iter_suffix)\n",
    "    if len(table_rows) > 0:\n",
    "        df = get_data_frame_from_rows(table_rows)\n",
    "        data_frames.append(df)\n",
    "        iter = iter + 1\n",
    "    iterate = len(table_rows) >= 25   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_df = pd.concat(data_frames, axis=0)\n",
    "\n",
    "## remove guys with score == 0\n",
    "red_df = agg_df.loc[agg_df['score'] > 0.0]\n",
    "red_df = red_df.sort_values(by='score', ascending=False)\n",
    "red_df.loc[red_df['team'] == '', 'team'] = 'NAN'\n",
    "red_df.to_csv(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sample code for one player\n",
    "\n",
    "# ## get projected score\n",
    "# ## have to change for historic data\n",
    "# score = table_rows[0].findAll('td', {'class': 'projected'})[0].string.strip()\n",
    "# name_card = table_rows[0].findAll('td', {'class', 'playerNameAndInfo'})[0].find('div')\n",
    "\n",
    "# player_name = name_card.find('a', {'class': 'playerName'}).string.strip()\n",
    "# pos, team = name_card.find('em').string.strip().split('-')\n",
    "# pos, team = pos.strip(), team.strip()\n",
    "\n",
    "# print(player_name, pos, team, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = {}\n",
    "\n",
    "# for row in table_rows:\n",
    "#     ## get projected score\n",
    "#     ## have to change for historic data\n",
    "#     score = row.findAll('td', {'class': 'projected'})[0].string.strip()\n",
    "#     name_card = row.findAll('td', {'class', 'playerNameAndInfo'})[0].find('div')\n",
    "\n",
    "#     player_name = name_card.find('a', {'class': 'playerName'}).string.strip()\n",
    "#     pos_team = name_card.find('em').string.strip().split('-')\n",
    "#     pos, team = pos_team[0].strip(), '' if len(pos_team) == 1 else pos_team[1].strip()\n",
    "    \n",
    "#     data[player_name] = {'pos': pos, 'team': team, 'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_frame = pd.DataFrame.from_dict(data, orient='index')\n",
    "# data_frame['score'] = data_frame['score'].astype('float')\n",
    "# data_frame.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
